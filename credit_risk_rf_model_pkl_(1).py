# -*- coding: utf-8 -*-
"""credit_risk_rf_model.pkl (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l9w5frZ_WYBzsjjBkDrCQYF8q9gFH7S4
"""

from ucimlrepo import fetch_ucirepo

# fetch dataset
statlog_german_credit_data = fetch_ucirepo(id=144)

# data (as pandas dataframes)
X = statlog_german_credit_data.data.features
y = statlog_german_credit_data.data.targets

# metadata
print(statlog_german_credit_data.metadata)

# variable information
print(statlog_german_credit_data.variables)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Display the first few rows of features and target
print("Features (X):")
display(X.head())

print("Target (y):")
display(y.head())

print("Feature Data Types:")
print(X.dtypes)

print("\nMissing Values in Features:")
print(X.isnull().sum())

print("\nTarget Data Types and Missing Values:")
print(y.dtypes)
print(y.isnull().sum())

# If y is a DataFrame with one column, get the column name
target_col = y.columns[0] if isinstance(y, pd.DataFrame) else y.name

plt.figure(figsize=(6,4))
sns.countplot(x=target_col, data=y)
plt.title('Distribution of Credit Risk (Target)')
plt.show()

print(y[target_col].value_counts())

print("Summary Statistics for Numerical Features:")
display(X.describe())

print(X.columns.tolist())

print(statlog_german_credit_data.variables)

column_mapping = {
    'Attribute1': 'checking_account_status',
    'Attribute2': 'duration_months',
    'Attribute3': 'credit_history',
    'Attribute4': 'purpose',
    'Attribute5': 'credit_amount',
    'Attribute6': 'savings_account_bonds',
    'Attribute7': 'employment_since',
    'Attribute8': 'installment_rate',
    'Attribute9': 'personal_status_sex',
    'Attribute10': 'other_debtors',
    'Attribute11': 'residence_since',
    'Attribute12': 'Property',
    'Attribute13': 'age',
    'Attribute14': 'other_installment_plans',
    'Attribute15': 'housing',
    'Attribute16': 'existing_credits',
    'Attribute17': 'job',
    'Attribute18': 'people_liable',
    'Attribute19': 'telephone',
    'Attribute20': 'foreign_worker'
}

X = X.rename(columns=column_mapping)

# Rename target column in y
y = y.rename(columns={'class': 'Target'})

# Example: Visualize distribution of 'duration_in_month' and 'credit_amount'
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
sns.histplot(X['duration_months'], bins=20, ax=axes[0], kde=True)
axes[0].set_title('Duration in Months')
sns.histplot(X['credit_amount'], bins=20, ax=axes[1], kde=True)
axes[1].set_title('Credit Amount')
plt.tight_layout()
plt.show()

target_col = 'Target'

plt.figure(figsize=(6,4))
   sns.countplot(x=cat_feature, hue=target_col, data=pd.concat([X, y], axis=1))
   plt.title(f'{cat_feature} vs Credit Risk')
   plt.show()

num_features = ['duration_months', 'credit_amount', 'age']
    for feature in num_features:
        plt.figure(figsize=(6,4))
        sns.histplot(X[feature], bins=20, kde=True)
        plt.title(f'Distribution of {feature}')
        plt.show()

        plt.figure(figsize=(6,4))
        sns.boxplot(x=y['Target'], y=X[feature])
        plt.title(f'{feature} by Credit Risk')
        plt.show()

cat_features = ['checking_account_status', 'credit_history', 'purpose', 'personal_status_sex', 'housing']
    for feature in cat_features:
        plt.figure(figsize=(8,4))
        sns.countplot(x=feature, hue='Target', data=pd.concat([X, y], axis=1))
        plt.title(f'{feature} vs Credit Risk')
        plt.xticks(rotation=45)
        plt.show()

print(y['Target'].value_counts())
sns.countplot(x='Target', data=y)
plt.title('Class Distribution')
plt.show()

print(X.isnull().sum())
print(y.isnull().sum())

plt.figure(figsize=(10,8))
sns.heatmap(X[num_features].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

display(X.describe())

# 1. One-hot encoding
X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)
print("Shape after encoding:", X_encoded.shape)
display(X_encoded.head())

# 2. Scaling
scaler = StandardScaler()
X_encoded[numerical_features] = scaler.fit_transform(X_encoded[numerical_features])
print("Summary statistics for scaled numerical features:")
display(X_encoded[numerical_features].describe())

# 3. Target encoding
y['Target'] = y['Target'].map({1: 0, 2: 1})
print("Target value counts after encoding:")
print(y['Target'].value_counts())

y['Target'] = y['Target'].map({1: 0, 2: 1})

print(y['Target'].unique())
print(y['Target'].isnull().sum())

# If you haven't mapped yet, do this:
y = y[y['Target'].isin([1, 2])]
X = X.loc[y.index]  # Keep X and y aligned

# Now map
y['Target'] = y['Target'].map({1: 0, 2: 1})

print(y['Target'].isnull().sum())

print(y['Target'].unique())
print(y.shape)

# If you still have the original column
if 'class' in y.columns:
    y['Target'] = y['class'].map({1: 0, 2: 1})
else:
    # If not, re-fetch the target from the dataset
    statlog_german_credit_data = fetch_ucirepo(id=144)
    y = statlog_german_credit_data.data.targets.rename(columns={'class': 'Target'})
    y['Target'] = y['Target'].map({1: 0, 2: 1})

y = y[y['Target'].isin([0, 1])]
X_encoded = X_encoded.loc[y.index]  # Align X with y

print(X_encoded.shape)
print(y.shape)
print(y['Target'].value_counts())

X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y['Target'], test_size=0.2, random_state=42, stratify=y['Target']
)

# 4. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y['Target'], test_size=0.2, random_state=42, stratify=y['Target']
)
print("Train set shape:", X_train.shape)
print("Test set shape:", X_test.shape)
print("Train target distribution:\n", y_train.value_counts())
print("Test target distribution:\n", y_test.value_counts())

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

# Train the model
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)

# Predict
y_pred = lr.predict(X_test)
y_proba = lr.predict_proba(X_test)[:, 1]

# Classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ROC-AUC Score
roc_auc = roc_auc_score(y_test, y_proba)
print("ROC-AUC Score:", roc_auc)

# ROC Curve
import matplotlib.pyplot as plt
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

# Train the Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict
y_pred_rf = rf.predict(X_test)
y_proba_rf = rf.predict_proba(X_test)[:, 1]

# Evaluation
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Random Forest Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
roc_auc_rf = roc_auc_score(y_test, y_proba_rf)
print("Random Forest ROC-AUC Score:", roc_auc_rf)

# ROC Curve
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_proba_rf)
plt.figure(figsize=(6,4))
plt.plot(fpr_rf, tpr_rf, label=f'ROC curve (area = {roc_auc_rf:.2f})')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest ROC Curve')
plt.legend()
plt.show()

importances = rf.feature_importances_
feature_names = X_train.columns
feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

# Display top 10 features
print("Top 10 Important Features:\n", feat_imp.head(10))

# Plot feature importance
plt.figure(figsize=(10,6))
feat_imp.head(15).plot(kind='bar')
plt.title('Top 15 Feature Importances (Random Forest)')
plt.ylabel('Importance Score')
plt.show()

# Get risk scores (probability of being 'bad' credit risk)
risk_scores = rf.predict_proba(X_test)[:, 1]

# Create a DataFrame with customer index, actual, predicted, and risk score
results_df = X_test.copy()
results_df['Actual'] = y_test.values
results_df['Predicted'] = y_pred_rf
results_df['Risk_Score'] = risk_scores

# Rank customers by risk score (descending: highest risk first)
results_df_sorted = results_df.sort_values(by='Risk_Score', ascending=False)

# Display top 10 highest risk customers
print("Top 10 Highest Risk Customers:")
display(results_df_sorted.head(10))

import joblib
joblib.dump(rf, 'credit_risk_rf_model.pkl')

pip install --upgrade gradio

